## 调度

kube-scheduler负责分配调度Pod到集群内的节点上，它监听kube-apiserver，查询还未分配Node的Pod，然后根据调度策略为这些Pod分配节点（更新Pod的NodeName字段）

### 调度器考虑因素

- 公平调度
- 资源高效利用
- QoS
- 亲和度
- 数据本地化
- 内部负载干扰
- deadlins

### 调度器调度阶段

kube-scheduler调度分为`过滤`和`打分`两个阶段

- 过滤：过滤不符合条件的节点。
- 打分：优先级排序，选择优先级最高的节点。

### 调度过滤策略

- PodFitsHostPorts：检查是否有Host Ports冲突
- PodFitsPorts：检查是否有端口冲突
- PodFitsResources：检查Node的资源是否充足，包括允许的Pod数量、CPU、内存、GPU个数以及其他OpaqueIntResources
- HostName：检查pod.Spec.NodeName是否与候选节点一致
- MatchNodeSelector：检查候选节点的pod.Spec.NodeSelector是否匹配。
- NoVolumeZoneConflict：检查volume zone是否冲突
- MatchInterPodAffinity：检查是否匹配Pod的亲和性要求
- NoDiskConflict：检查是否存在Volume冲突，仅限于GCE PD、AWS EBS、Ceph RBD以及iSCSI
- PodToleratersNodeTaints：检查Pod是否容忍Node Taints
- CheckNodeMemoryPressure：检查Pod是否可以调度到MemoryPressure的节点上
- CheckNodeDiskPressure：检查Pod是否可以调度到DiskPressure的节点上
- NoVolumeNodeConflict：检查节点是否满足Pod所引用的Volume的条件

### 调度打分策略

- SelectorSpreadPriority：优先减少节点上属于同一个Service或Replication Controller的Pod数量
- InterPodAffinityPriority：优先将Pod调度到相同的拓扑上（如同一个节点、Rack、Zone等）
- LeastRequestedPriority：优先调度到请求资源少的节点上
- BalanceResourceAllocation：优先平衡各节点的资源使用
- NodePreferAvoidPodsPriority：alpha.kubernetes.io/preferAvoidPods字段判断，权重为10000，避免其他优先级策略的影响
- NodeAffinityPriority：优先调度到匹配NodeAffinity的节点上
- TaintTolerationPriority：优先调度到匹配TaintToleration的节点上
- ServiceSpreadingPriority：尽量将同一个service的Pod分布到不同节点上，已经被SelectorSpreadPriority替代（默认未使用）
- EqualPriority：将所有节点的优先级设置为1（默认未使用）
- ImageLocalityPriority：尽量使用大镜像的容器调度到已经下拉了该镜像的节点上（默认未使用）
- MostRequestedPriority：尽量调度到已经使用过的Node上，特别适用于cluster-autoscaler（默认未使用）

### 调度场景

#### 把Pod调度到指定node上

可以通过nodeSelector、nodeAffinity、podAffinity以及Taints和tolerations等来将Pod调度到需要的Node上。

也可以通过设置nodeName参数，将Pod调度到指定node节点上。

##### nodeSelector

```shell
# 首先给Node打上标签
kubectl lable nodes node-01 disktype=ssd

# 然后在daemonset中指定nodeSelector为disktype=ssd
spec:
    nodeSelector:
        disktype: ssd
```

##### nodeAffinity

- 必须满足的亲和性条件
- 亲和性优选条件

##### podAffinity

podAffinity基于pod的标签来选择Node，仅调度到满足条件pod所在的Node上。

 ##### taints和tolerations

taints和tolerations用于保证pod不被调度到不合适的Node上，其中taint应用于Node上，而tolerations则应用于pod上。

taint类型：

- NoSchedule：新的Pod不调度到该Node上，不影响正在运行的Pod
- PreferNoSchedule：soft版的NodeSchedule，尽量不调度到该Node上
- NoExecute：新的Pod不调度到该上，并且删除已在运行的Pod。Pod可以增加一个时间（tolerationSeconds）

**给Node打上taint，在deployment yaml配置tolerations**

### kube-scheduler 

```go
// https://cncamp.notion.site/kube-scheduler-0d45b37a5c9a46008aaf9f9e2088b3ce

// Framework manages the set of plugins in use by the scheduling framework.
// Configured plugins are called at specified points in a scheduling context.
type Framework interface {
	Handle
	QueueSortFunc() LessFunc
	RunPreFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod) *Status
	RunPostFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, filteredNodeStatusMap NodeToStatusMap) (*PostFilterResult, *Status)
	RunPreBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status
	RunPostBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string)
	RunReservePluginsReserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status
	RunReservePluginsUnreserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string)
	RunPermitPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status
	WaitOnPermit(ctx context.Context, pod *v1.Pod) *Status
	RunBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status
	HasFilterPlugins() bool
	HasPostFilterPlugins() bool
	HasScorePlugins() bool
	ListPlugins() *config.Plugins
	ProfileName() string
}

Schedule()-->
	// filter
	g.findNodesThatFitPod(ctx, extenders, fwk, state, pod)-->
		// 1.filter预处理阶段：遍历pod的所有initcontainer和主container，计算pod的总资源需求
		s := fwk.RunPreFilterPlugins(ctx, state, pod) // e.g. computePodResourceRequest
		// 2. filter阶段，遍历所有节点，过滤掉不符合资源需求的节点
		g.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, allNodes)-->
			fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo)-->
				s, err := getPreFilterState(cycleState)
				insufficientResources := fitsRequest(s, nodeInfo, f.ignoredResources, f.ignoredResourceGroups)
		// 3. 处理扩展plugin
		findNodesThatPassExtenders(extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
		// score
	prioritizeNodes(ctx, extenders, fwk, state, pod, feasibleNodes)-->
		// 4. score，比如处理弱亲和性，将preferredAffinity语法进行解析
		fwk.RunPreScorePlugins(ctx, state, pod, nodes) // e.g. nodeAffinity
		fwk.RunScorePlugins(ctx, state, pod, nodes)-->
		// 5. 为节点打分
			f.runScorePlugin(ctx, pl, state, pod, nodeName) // e.g. noderesource fit
		// 6. 处理扩展plugin
		extenders[extIndex].Prioritize(pod, nodes)
		// 7.选择节点
		g.selectHost(priorityList)
sched.assume(assumedPod, scheduleResult.SuggestedHost)-->
	  // 8.假定选中pod
	sched.SchedulerCache.AssumePod(assumed)-->
fwk.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)-->
	f.runReservePluginReserve(ctx, pl, state, pod, nodeName) // e.g. bindVolume。其实还没大用
runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)-->
	f.runPermitPlugin(ctx, pl, state, pod, nodeName) // empty hook
fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) // 同 runReservePluginReserve
		// bind
		// 9.绑定pod
sched.bind(bindingCycleCtx, fwk, assumedPod, scheduleResult.SuggestedHost, state)-->
	f.runBindPlugin(ctx, bp, state, pod, nodeName)-->
		b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions{})-->
			return c.client.Post().Namespace(c.ns).Resource("pods").Name(binding.Name).VersionedParams(&opts, scheme.ParameterCodec).SubResource("binding").Body(binding).Do(ctx).Error()
```

